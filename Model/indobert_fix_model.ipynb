{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "JYg8N0Fq_fKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1752c18-187b-4ae6-9ef7-9c4cdfc9251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "-L51q-Az_nVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a23f2d9-3e6f-4ecd-cf3e-25812d4f6a6a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "SHgsi2yT_pAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88fc0ee-b2c6-4bf4-85d3-acca9686de38"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Tugas Akhir/dataset_with_spelling_check.csv\")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Ek5-_2xA_q3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8473cdfe-4c60-4565-d82b-3cb8621faeb0"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            komentar  label\n",
              "0  tukang ngibul jangan bagaimana enggak mau semu...      1\n",
              "1  seolah negara hanya milik pkb saja parah dari ...      0\n",
              "2  ketua ok dpp partai gerindra juga mengatakan a...      1\n",
              "3  cara pandang ucapannya kodari kacau rusak demo...      1\n",
              "4  harus tolak enggak ada yang nama nya apalagi p...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd264e5b-c661-4606-85a8-df5d9866dc48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>komentar</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tukang ngibul jangan bagaimana enggak mau semu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>seolah negara hanya milik pkb saja parah dari ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ketua ok dpp partai gerindra juga mengatakan a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cara pandang ucapannya kodari kacau rusak demo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>harus tolak enggak ada yang nama nya apalagi p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd264e5b-c661-4606-85a8-df5d9866dc48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd264e5b-c661-4606-85a8-df5d9866dc48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd264e5b-c661-4606-85a8-df5d9866dc48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data['komentar']\n",
        "labels = data['label']"
      ],
      "metadata": {
        "id": "k0oKJ_M7ATNX"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')"
      ],
      "metadata": {
        "id": "hNBd49PvAV2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034ea763-d0c6-457b-d0fe-1662387129fe"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p2', # Use the 12-layer BERT model, with an cased vocab.\n",
        "    num_labels = 2, \n",
        "    output_attentions = False, # return attentions weights\n",
        "    output_hidden_states = False, # returns all hidden-states\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# memberi tau Pytorch untuk menjalankan di GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "2W_FUihFAYWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2843670-a1af-4c63-fe4a-c009c5d32f4c"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "sent_length = []\n",
        "\n",
        "for sent in sentences:\n",
        "    # Tokenize data dan tambahkan token `[CLS]` dan `[SEP]`.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    sent_length.append(len(input_ids))\n",
        "\n",
        "print('Average length = ', sum(sent_length)/len(sent_length)) # menghitung average panjang kalimat\n",
        "print('Median length = ', statistics.median(sent_length)) # menghitung nilai tengah"
      ],
      "metadata": {
        "id": "QlqkNUmtAb3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4d2ac2-6150-4239-8183-e5b29d91516a"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length =  18.16103927688379\n",
            "Median length =  15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "test_size = 0.2\n",
        "\n",
        "# membagi data secara acak\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(data['komentar'], data['label'], random_state = 1109, test_size=test_size, stratify=data['label'])\n",
        "\n",
        "valid_text, test_text, valid_labels, test_labels = train_test_split(val_text, val_labels, random_state = 42, test_size=test_size, stratify=val_labels)\n",
        "\n",
        "\n",
        "print(' training samples',len(train_text))\n",
        "print(' validation samples', len(valid_text))\n",
        "print('test samples', len(test_text))"
      ],
      "metadata": {
        "id": "0Liprq98Agwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edca9f7b-8075-45f0-ce37-a2a1e3a5b0ba"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " training samples 19736\n",
            " validation samples 3948\n",
            "test samples 987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = 30,\n",
        "    padding= 'max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask = True,   # membangun attn. masks.\n",
        "    return_tensors = 'pt',     # kembalikan menjadi pytorch tensors.\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    valid_text.tolist(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = 30,\n",
        "    padding= 'max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask = True,   # membangun attn. masks.\n",
        "    return_tensors = 'pt',     # kembalikan menjadi pytorch tensors.\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    add_special_tokens = True,\n",
        "    max_length = 30,\n",
        "    padding= 'max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask = True,   # membangun attn. masks.\n",
        "    return_tensors = 'pt',     # kembalikan menjadi pytorch tensors.\n",
        ")"
      ],
      "metadata": {
        "id": "KJ3pcMtJXuYc"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(valid_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8val9IvWfedE",
        "outputId": "1f71353f-916c-4978-9d2d-5e7727e0ff47"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-312-faa6f40e035f>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
            "<ipython-input-312-faa6f40e035f>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
            "<ipython-input-312-faa6f40e035f>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
            "<ipython-input-312-faa6f40e035f>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
            "<ipython-input-312-faa6f40e035f>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
            "<ipython-input-312-faa6f40e035f>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_mask = torch.tensor(tokens_test['attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids = []\n",
        "# attention_masks = []\n",
        "\n",
        "# for sent in sentences:\n",
        "#     encoded_dict = tokenizer.encode_plus(\n",
        "#                         sent,                      # kalimat yang akan di encode\n",
        "#                         add_special_tokens = True, # Menambahkan token '[CLS]' dan '[SEP]'\n",
        "#                         max_length = 30,           # Pad & truncate semua kalimat.\n",
        "#                         padding = \"max_length\",\n",
        "#                         truncation=True,\n",
        "#                         return_attention_mask = True,   # membangun attn. masks.\n",
        "#                         return_tensors = 'pt',     # kembalikan menjadi pytorch tensors.\n",
        "#                    )\n",
        "    \n",
        "#     input_ids.append(encoded_dict['input_ids'])\n",
        "#     attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # gabungkan semua tensor\n",
        "# input_ids = torch.cat(input_ids, dim=0)\n",
        "# attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels.tolist())\n",
        "\n",
        "# print('Original: ', sentences[0])\n",
        "# print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "id": "F6HhU_l1Aevi"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# membuat dataloader untuk training dan validation sets. \n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_data,  # sampel data untuk melakukan training\n",
        "            sampler = RandomSampler(train_data), # memilih batch secara acak\n",
        "            batch_size = batch_size # melakukan training dengan batch size yang sudah ditentukan\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_data, # sampel data untuk melakukan validasi\n",
        "            sampler = SequentialSampler(val_data), # memilih batch secara acak\n",
        "            batch_size = batch_size #  melakukan evaluasi dengan batch size yang sudah ditentukan\n",
        "        )"
      ],
      "metadata": {
        "id": "MY6RfQZQAkXI"
      },
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8\n",
        "                )"
      ],
      "metadata": {
        "id": "poCAlz43An8M"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 12\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "print('Jumlah batch :', len(train_dataloader))\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# membuat learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "3upFXeJUAqj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fd59ca-57f3-4518-dd24-fc8fb48f6786"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah batch : 617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung akurasi hasil prediksi dibandingkan dengan label asli\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() #menjadikan bentuknya jadi satu dimensi\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "aMozRxDaAsX_"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "EabSymoIAwah"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset total loss setiap epoch.\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    # training model\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update setiap 50 batch\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a backward pass\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # token_type_ids is same as the \"segment ids\", which differentiates \n",
        "        # sentence 1 and 2 in sentence-pair tasks\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None,\n",
        "                             return_dict = False,\n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
        "    \n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode (batchnorm, dropout disable)\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Deactivate autograd, it will reduce memory usage and speed up computations\n",
        "        # but you won’t be able to backprop (which you don’t want in an eval script).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   return_dict = False,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.5f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.5f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    #save the best model\n",
        "    if avg_val_loss < best_valid_loss:\n",
        "        best_valid_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'Epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Accuracy': avg_train_accuracy,\n",
        "            'Validation Loss': avg_val_loss,\n",
        "            'Validation Accuracy': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "-iFj61R-Azo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec69ff5-915c-4fad-d9f3-46a0959a1ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:09.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:18.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:37.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:46.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:56.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:05.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:14.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:24.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:34.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:43.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:53.\n",
            "\n",
            "  Average training loss: 0.10733\n",
            "  Training epoch took: 0:01:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95741\n",
            "  Validation Loss: 0.15879\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:48.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:07.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:17.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:36.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:46.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.08415\n",
            "  Training epoch took: 0:01:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94355\n",
            "  Validation Loss: 0.23113\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:49.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.06813\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94666\n",
            "  Validation Loss: 0.25668\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:49.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.04848\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95002\n",
            "  Validation Loss: 0.23200\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:49.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.04040\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95060\n",
            "  Validation Loss: 0.27028\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:48.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.03018\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95346\n",
            "  Validation Loss: 0.26198\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:48.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:56.\n",
            "\n",
            "  Average training loss: 0.02199\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95212\n",
            "  Validation Loss: 0.29181\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:49.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:27.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n",
            "  Batch   550  of    617.    Elapsed: 0:01:47.\n",
            "  Batch   600  of    617.    Elapsed: 0:01:57.\n",
            "\n",
            "  Average training loss: 0.01649\n",
            "  Training epoch took: 0:02:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95094\n",
            "  Validation Loss: 0.30950\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "Training...\n",
            "  Batch    50  of    617.    Elapsed: 0:00:10.\n",
            "  Batch   100  of    617.    Elapsed: 0:00:19.\n",
            "  Batch   150  of    617.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    617.    Elapsed: 0:00:39.\n",
            "  Batch   250  of    617.    Elapsed: 0:00:49.\n",
            "  Batch   300  of    617.    Elapsed: 0:00:58.\n",
            "  Batch   350  of    617.    Elapsed: 0:01:08.\n",
            "  Batch   400  of    617.    Elapsed: 0:01:18.\n",
            "  Batch   450  of    617.    Elapsed: 0:01:28.\n",
            "  Batch   500  of    617.    Elapsed: 0:01:37.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 5)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('Epoch')\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "j7dGABHuA3Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(df_stats['Training Accuracy'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Validation Accuracy'], 'g-o', label=\"Validation\")\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RgHqeRomBEaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Validation Loss'], 'g-o', label=\"Validation\")\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fx6Qt7oC2RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_indobert = '/content/drive/My Drive/Tugas Akhir/model/'\n",
        "\n",
        "print(\"Saving model to %s\" % model_indobert)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model \n",
        "model_to_save.save_pretrained(model_indobert)\n",
        "tokenizer.save_pretrained(model_indobert)"
      ],
      "metadata": {
        "id": "DWafJ_vaBF8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "CX4ledEPEOib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  outputs = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = outputs[0]\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "VNvo19g4gE9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "UhxpC3GMnZse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "id": "5exUMA4mBIS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "metadata": {
        "id": "eSbxwtx0gS1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}